{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import rasterio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from skimage.transform import resize\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import Swin2SRImageProcessor, Swin2SRForImageSuperResolution\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "hr_size = (120,120) \n",
    "lr_size = (60,60)\n",
    "\n",
    "class SRDataset(Dataset):\n",
    "    def __init__(self, data_paths, lr_size=(64, 64), hr_size=(128, 128)):\n",
    "        self.data_paths = data_paths\n",
    "        self.lr_size = lr_size\n",
    "        self.hr_size = hr_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.data_paths[idx]\n",
    "        \n",
    "        img = Image.open(image_path).convert('RGB')  \n",
    "        \n",
    "        img = np.array(img) \n",
    "        \n",
    "        lr_img = resize(img, self.lr_size, anti_aliasing=True, preserve_range=True)  \n",
    "        hr_img = resize(img, self.hr_size, anti_aliasing=True, preserve_range=True) \n",
    "\n",
    "        lr_img = lr_img / 255.0\n",
    "        hr_img = hr_img / 255.0\n",
    "\n",
    "\n",
    "        lr_tensor = torch.tensor(lr_img, dtype=torch.float32).permute(2, 0, 1)  # [3, 64, 64]\n",
    "        hr_tensor = torch.tensor(hr_img, dtype=torch.float32).permute(2, 0, 1)  # [3, 128, 128]\n",
    "\n",
    "        return lr_tensor, hr_tensor, image_path\n",
    "\n",
    "def filter_120x120_images(datasetPath):\n",
    "    imagePaths = []\n",
    "    for root, dirs, files in os.walk(datasetPath):\n",
    "        for file in files:\n",
    "            image_path = os.path.join(root, file)\n",
    "            with rasterio.open(image_path) as src:\n",
    "                if src.width == 120 and src.height == 120:\n",
    "                    imagePaths.append(image_path)\n",
    "                else:\n",
    "                    print(f\"Skipping image {file} with size {src.width}x{src.height}\")\n",
    "\n",
    "    return imagePaths\n",
    "\n",
    "datasetPath = r\"SampleDataset\"\n",
    "imagePaths = filter_120x120_images(datasetPath)\n",
    "\n",
    "dataset = SRDataset(imagePaths, lr_size, hr_size)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "lr_img, hr_img, pths = next(iter(dataloader))\n",
    "\n",
    "lr_img = lr_img.permute(0, 2, 3, 1).numpy() \n",
    "hr_img = hr_img.permute(0, 2, 3, 1).numpy() \n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(lr_img[0])  \n",
    "plt.title(\"Low-res Image\")\n",
    "plt.axis('off') \n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(hr_img[0])  \n",
    "plt.title(\"High-res Image\")\n",
    "plt.axis('off') \n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size=60, patch_size=6, in_channels=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)  # [B, embed_dim, num_patches_per_row, num_patches_per_col]\n",
    "        x = x.flatten(2).transpose(1, 2)  # [B, num_patches, embed_dim]\n",
    "        return x\n",
    "\n",
    "class SwinTransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim=768, num_heads=12, mlp_ratio=4.0, drop=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=drop)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, int(embed_dim * mlp_ratio)),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(int(embed_dim * mlp_ratio), embed_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0]\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class Swin2SR(nn.Module):\n",
    "    def __init__(self, img_size=60, patch_size=6, in_channels=3, embed_dim=768, depth=12, num_heads=12, scale_factor=2):\n",
    "        super().__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.in_channels = in_channels\n",
    "        \n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, (img_size // patch_size) ** 2, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(p=0.1)\n",
    "        \n",
    "        self.blocks = nn.ModuleList([SwinTransformerBlock(embed_dim, num_heads) for _ in range(depth)])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        self.upscaled_patch_size = patch_size * scale_factor\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Linear(embed_dim, (patch_size * scale_factor) ** 2 * in_channels),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "        self.refine = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, 3, 1, 1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(64, in_channels, 3, 1, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        \n",
    "        num_patches = (self.img_size // self.patch_size) ** 2\n",
    "        x = x.view(B, num_patches, self.in_channels, self.upscaled_patch_size, self.upscaled_patch_size)\n",
    "\n",
    "        patches_per_row = self.img_size // self.patch_size\n",
    "        rows = []\n",
    "        for i in range(patches_per_row):\n",
    "            row_patches = []\n",
    "            for j in range(patches_per_row):\n",
    "                patch_idx = i * patches_per_row + j\n",
    "                row_patches.append(x[:, patch_idx, :, :, :])\n",
    "            row = torch.cat(row_patches, dim=3)\n",
    "            rows.append(row)\n",
    "        x = torch.cat(rows, dim=2)\n",
    "        x = self.refine(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "swin2sr_model = Swin2SR(\n",
    "    img_size=60,\n",
    "    patch_size=6,\n",
    "    in_channels=3,\n",
    "    embed_dim=768,\n",
    "    depth=12,\n",
    "    num_heads=12,\n",
    "    scale_factor=2 \n",
    ").to(device)\n",
    "\n",
    "x = torch.randn(1, 3, 60, 60).to(device)\n",
    "output = swin2sr_model(x)\n",
    "print(output.shape) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "def psnr_loss(output, target, max_pixel_value=1.0):\n",
    "    mse = torch.mean((output - target) ** 2)\n",
    "    psnr = 20 * torch.log10(max_pixel_value / torch.sqrt(mse))\n",
    "    return -psnr  # Return negative PSNR to minimize\n",
    "from torchmetrics.functional import structural_similarity_index_measure as ssim\n",
    "\n",
    "class PSNR_SSIM_Loss(nn.Module):\n",
    "    def __init__(self, alpha=0.5):\n",
    "        super(PSNR_SSIM_Loss, self).__init__()\n",
    "        self.alpha = alpha \n",
    "    \n",
    "    def forward(self, output, target):\n",
    "        psnr_value = psnr_loss(output, target)\n",
    "        ssim_value = 1 - ssim(output, target)\n",
    "        return self.alpha * psnr_value + (1 - self.alpha) * ssim_value\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vit_model = Swin2SR(img_size=60, patch_size=6, in_channels=3, embed_dim=768, depth=12, num_heads=12)\n",
    "vit_model.to(device)\n",
    "criterion = PSNR_SSIM_Loss(alpha=0.7) \n",
    "optimizer = optim.Adam(vit_model.parameters(), lr=1e-4)\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    vit_model.train()\n",
    "    epoch_loss = 0.0\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    for batch_idx, (lr_tensor, hr_tensor, pths) in enumerate(progress_bar):\n",
    "        lr_tensor, hr_tensor = lr_tensor.to(device), hr_tensor.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = vit_model(lr_tensor)\n",
    "\n",
    "        if outputs.shape != hr_tensor.shape:\n",
    "            print(f\"Output shape: {outputs.shape}, HR shape: {hr_tensor.shape}\")\n",
    "            raise ValueError(\"Output shape does not match HR shape\")\n",
    "\n",
    "        loss = criterion(outputs, hr_tensor)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        progress_bar.set_postfix({\"Loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(dataloader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vit_model.state_dict(), \"vit_model2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "vit_model = Swin2SR(img_size=60, patch_size=6, in_channels=3, embed_dim=768, depth=12, num_heads=12)\n",
    "\n",
    "vit_model.load_state_dict(torch.load(\"vit_model2.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(dataloader))[2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rejoin list\n",
    "\"_\".join(next(iter(dataloader))[2][0].split(\"_\")[:-2]).split(\"\\\\\")[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from skimage.metrics import peak_signal_noise_ratio, structural_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "vit_model.eval()\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vit_model.to(device)\n",
    "\n",
    "# MSE Loss function\n",
    "mse_criterion = torch.nn.MSELoss()\n",
    "\n",
    "def calculate_psnr(sr_img, hr_img):\n",
    "    # PSNR calculation (using skimage for precision)\n",
    "    sr_img_np = sr_img.cpu().detach().numpy()\n",
    "    hr_img_np = hr_img.cpu().detach().numpy()\n",
    "    psnr_value = peak_signal_noise_ratio(hr_img_np, sr_img_np, data_range=1.0)\n",
    "    return psnr_value\n",
    "\n",
    "def calculate_ssim(sr_img, hr_img):\n",
    "    # SSIM calculation (using skimage) with a smaller window size (3x3)\n",
    "    sr_img_np = sr_img.permute(1, 2, 0).cpu().detach().numpy()\n",
    "    hr_img_np = hr_img.permute(1, 2, 0).cpu().detach().numpy()\n",
    "    ssim_value = structural_similarity(hr_img_np, sr_img_np, multichannel=True, data_range=1.0, win_size=3) # need to explicitly state 3 because of the small image size\n",
    "    return ssim_value\n",
    "\n",
    "def visualize_super_resolve_dataloader(dataloader):\n",
    "    psnr = 0.0\n",
    "    ssim = 0.0\n",
    "    mse = 0.0\n",
    "\n",
    "    for batch_idx, (lr_tensor, hr_tensor, pths) in enumerate(tqdm(dataloader)):\n",
    "        lr_tensor, hr_tensor = lr_tensor.to(device), hr_tensor.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            sr_image = vit_model(lr_tensor) \n",
    "        for i in range(lr_tensor.shape[0]): \n",
    "            #plt.figure(figsize=(12, 6))\n",
    "            #plt.subplot(1, 3, 1)\n",
    "            lr_img = lr_tensor[i].permute(1, 2, 0).cpu().numpy() \n",
    "            #plt.imshow(lr_img)\n",
    "            #plt.title(f\"Low-Resolution: {lr_img.shape[:2]}\")\n",
    "\n",
    "            sr_img = sr_image[i]\n",
    "            #plt.subplot(1, 3, 2)\n",
    "            sr_img_np = sr_img.permute(1, 2, 0).cpu().detach().numpy()\n",
    "            #plt.imshow(sr_img_np)\n",
    "            #plt.title(f\"Super-Resolved: {sr_img_np.shape[:2]}\")\n",
    "\n",
    "            hr_img = hr_tensor[i]\n",
    "            #plt.subplot(1, 3, 3)\n",
    "            hr_img_np = hr_img.permute(1, 2, 0).cpu().numpy()\n",
    "            #plt.imshow(hr_img_np)\n",
    "            #plt.title(f\"High-Resolution (Ground Truth): {hr_img_np.shape[:2]}\")\n",
    "\n",
    "            psnr_value = calculate_psnr(sr_img, hr_img)\n",
    "            ssim_value = calculate_ssim(sr_img, hr_img)  # Updated with win_size=3\n",
    "            mse_value = mse_criterion(sr_img, hr_img).item()\n",
    "\n",
    "            #print(f\"PSNR: {psnr_value:.2f}, SSIM: {ssim_value:.4f}, MSE: {mse_value:.4f}\")\n",
    "\n",
    "            psnr += psnr_value\n",
    "            ssim += ssim_value\n",
    "            mse += mse_value\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "    num_samples = len(dataloader) * dataloader.batch_size\n",
    "    print(f\"Average PSNR: {psnr/num_samples:.2f}\")\n",
    "    print(f\"Average SSIM: {ssim/num_samples:.4f}\")\n",
    "    print(f\"Average MSE: {mse/num_samples:.4f}\")\n",
    "\n",
    "\n",
    "# Call the function with your DataLoader\n",
    "visualize_super_resolve_dataloader(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def super_resolve_images(dataloader, output_dir=\"SR_Basic\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    vit_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (lr_tensor, hr_tensor, paths) in enumerate(tqdm(dataloader)):\n",
    "            if lr_tensor.shape[1] == 1:\n",
    "                lr_tensor = lr_tensor.repeat(1, 3, 1, 1)\n",
    "\n",
    "            lr_tensor = lr_tensor.to(device)\n",
    "            sr_images = vit_model(lr_tensor)\n",
    "\n",
    "            for i in range(lr_tensor.shape[0]):\n",
    "                # Convert and process images\n",
    "                lr_img = lr_tensor[i].permute(1, 2, 0).cpu().numpy()\n",
    "                sr_img = sr_images[i].permute(1, 2, 0).cpu().detach().numpy()\n",
    "\n",
    "                lr_img = np.clip(lr_img, 0, 1)\n",
    "                sr_img = np.clip(sr_img, 0, 1)\n",
    "\n",
    "                # Display images\n",
    "                plt.figure(figsize=(10, 5))\n",
    "                plt.subplot(1, 2, 1)\n",
    "                plt.imshow(lr_img)\n",
    "                plt.title(f\"Low-Resolution: {lr_img.shape[:2]}\")\n",
    "                plt.subplot(1, 2, 2)\n",
    "                plt.imshow(sr_img)\n",
    "                plt.title(f\"Super-Resolved: {sr_img.shape[:2]}\")\n",
    "\n",
    "                # Get the current path\n",
    "                current_path = paths[i] if isinstance(paths[i], str) else paths[i][0]\n",
    "                \n",
    "                # Extract base filename while preserving numbers\n",
    "                base_name = os.path.splitext(os.path.basename(current_path))[0]\n",
    "                # base_name will be like 'S2A_MSIL2A_20170613T101031_N9999_R022_T33UUP_40_65'\n",
    "                \n",
    "                # Save with original numbers intact\n",
    "                lr_save_path = os.path.join(output_dir, f\"{base_name}_LR.png\")\n",
    "                sr_save_path = os.path.join(output_dir, f\"{base_name}_SR.png\")\n",
    "\n",
    "                # Save images\n",
    "                Image.fromarray((lr_img * 255).astype(np.uint8)).save(lr_save_path)\n",
    "                Image.fromarray((sr_img * 255).astype(np.uint8)).save(sr_save_path)\n",
    "\n",
    "                print(f\"Saved images:\\n  LR: {lr_save_path}\\n  SR: {sr_save_path}\")\n",
    "\n",
    "    return lr_img, sr_img\n",
    "# Usage\n",
    "lr_img, sr_img = super_resolve_images(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for (lr_tensor, hr_tensor, paths) in dataloader:\n",
    "    print(paths)\n",
    "    if (i >= 10):\n",
    "        i += 1\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
